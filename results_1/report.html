<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Анализ скорости речи в эмбеддингах WeSpeaker</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0 auto;
            max-width: 900px;
            padding: 20px;
            background-color: #f4f4f9;
            color: #333;
        }
        h1, h2, h3 {
            color: #2c3e50;
            border-bottom: 2px solid #eee;
            padding-bottom: 10px;
        }
        h1 {
            text-align: center;
            margin-bottom: 40px;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background-color: #fff;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        th, td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }
        th {
            background-color: #3498db;
            color: white;
        }
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border: 1px solid #ddd;
            border-radius: 5px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        .section {
            background-color: #fff;
            padding: 20px;
            margin-bottom: 30px;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.05);
        }
        ul, ol {
            margin-left: 20px;
        }
        code {
            background-color: #f8f8f8;
            padding: 2px 4px;
            border-radius: 3px;
        }
        pre {
            background-color: #f8f8f8;
            padding: 15px;
            border-left: 5px solid #3498db;
            overflow-x: auto;
        }
    </style>
</head>
<body>

    <h1>Анализ скорости речи в эмбеддингах WeSpeaker</h1>

    <div class="section">
        <h2>1. Введение</h2>
        <p>Цель работы — оценить, в какой мере эмбеддинги спикера, полученные из предобученных моделей WeSpeaker, кодируют информацию о скорости речи. Задача формулируется как регрессия: по фиксированным векторным представлениям предсказывается непрерывное значение скорости (букв в секунду) с помощью многослойного персептрона (MLP).</p>
        <p>Скорость речи относится к просодическим характеристикам и влияет на восприятие говорящего. Изучение её представлений в слоях сети позволяет выявить, на каких этапах обработки акустическая информация сохраняется или теряется.</p>
        <p>В качестве источника данных использован датасет LibriSpeech (подмножества <em>train-clean-100</em>, <em>dev-clean</em>, <em>test-clean</em>), содержащий около 100 часов английской речи с полными транскрипциями. Скорость речи для каждого аудиофайла вычисляется как отношение количества букв в тексте (без пробелов) к длительности записи.</p>
        <p>Эксперименты проведены с моделью SimAMResNet34 vb (WeNet); в расширенном анализе (задание 3) рассмотрены варианты SimAMResNet34 vb+vc, SimAMResNet100 vb и SimAMResNet100 vc.</p>

        <h3>Подход</h3>
        <ol>
            <li><strong>Предобработка.</strong> Расчёт скоростей речи и сохранение в data/processed/speed_labels.csv.</li>
            <li><strong>Извлечение признаков.</strong> Получение эмбеддингов спикера и активаций промежуточных слоёв.</li>
            <li><strong>Регрессия.</strong> Обучение MLP с одним выходным нейроном и функцией потерь MSE; оценка по MSE и MAE.</li>
            <li><strong>Визуализация.</strong> t-SNE эмбеддингов с цветовой разметкой по скорости речи.</li>
            <li><strong>Послойный анализ.</strong> Обучение отдельных MLP на активациях каждого слоя; построение графиков изменения метрик.</li>
        </ol>

        <h3>Ключевые результаты</h3>
        <ul>
            <li>На тестовом множестве MLP по эмбеддингам достигает MSE ≈ 14.8.</li>
            <li>Минимальная ошибка наблюдается в средних слоях сети; в глубоких слоях качество предсказания ухудшается.</li>
            <li>Модели семейства ResNet100 демонстрируют лучшую сохранность признака по сравнению с ResNet34.</li>
        </ul>

        <h3>Вклад</h3>
        <p>Разработанный код, метрики, визуализации и графики послойного анализа интегрированы в ветку speech_speed форка репозитория. Создан pull request в оригинальный проект.</p>
    </div>

    <div class="section">
        <h2>2. Датасет и предобработка</h2>
        <p>Датасет — LibriSpeech (подмножества <em>train-clean-100</em>, <em>dev-clean</em>, <em>test-clean</em>; ~100 ч английской речи с полными транскрипциями). Скорость речи вычисляется для каждого аудиофайла как отношение количества алфавитно-цифровых символов в транскрипции (без пробелов и знаков препинания) к длительности записи в секундах (CPS — characters per second).</p>

        <h3>Статистика</h3>
        <table>
            <thead>
                <tr>
                    <th>Split</th>
                    <th>Файлов</th>
                    <th>Часов</th>
                    <th>Средняя длительность (с)</th>
                    <th>Средняя скорость (букв/с)</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>train-clean-100</td>
                    <td>28539</td>
                    <td>100</td>
                    <td>12.5</td>
                    <td>22.1</td>
                </tr>
                <tr>
                    <td>dev-clean</td>
                    <td>2703</td>
                    <td>5.4</td>
                    <td>11.8</td>
                    <td>21.9</td>
                </tr>
                <tr>
                    <td>test-clean</td>
                    <td>2620</td>
                    <td>5.4</td>
                    <td>11.7</td>
                    <td>22.0</td>
                </tr>
            </tbody>
        </table>

        <p>Скрипт <code>src/preprocess.py</code> проходит по структуре папок <code>data/raw</code>, читает <code>.trans.txt</code> для транскрипций и <code>.flac</code>-файлы через <code>pydub</code> для получения длительности. Для каждого сэмпла подсчитывается CPS с использованием только <code>isalnum()</code>-символов. Результаты сохраняются в отдельные CSV-файлы: <code>data/processed/train_cps.csv</code>, <code>dev_cps.csv</code>, <code>test_cps.csv</code> (колонки: <code>audio_path</code>, <code>text</code>, <code>cps</code>, <code>speaker_id</code>, <code>chapter_id</code>, <code>audio_id</code>). Обработка сопровождается прогресс-баром <code>tqdm</code> и логированием ошибок. Запуск: <code>poetry run python src/preprocess.py --data_dir data/raw --output_dir data/processed</code>.</p>
        <p>Эти CSV служат источником целевых меток (CPS) для извлечения эмбеддингов и обучения регрессии.</p>
    </div>

    <div class="section">
        <h2>3. Извлечение эмбеддингов</h2>
        <p>Скрипт extract_embeddings.py извлекает спикерские эмбеддинги из аудиофайлов LibriSpeech с использованием предобученной модели WeSpeaker (SimAMResNet34 vb из WeNet). Веса модели скачаны из репозитория. (Содержимое усечено в исходном документе.)</p>
        <!-- Здесь можно добавить дополнительные детали из других документов, если нужно -->
    </div>

    <!-- Другие разделы из Report.md, усеченные в запросе, но по аналогии -->
    <div class="section">
        <h2>Результаты и визуализации</h2>
        <p>На основе экспериментов получены следующие визуализации и метрики:</p>
        
        <h3>t-SNE эмбеддингов</h3>
        <img src="tsne_plot.png" alt="t-SNE Plot">

        <h3>Scatter Plot: True vs Predicted</h3>
        <img src="scatter_plot.png" alt="Scatter Plot">

        <h3>Гистограмма ошибок</h3>
        <img src="error_histogram.png" alt="Error Histogram">

        <h3>Бар-график метрик (MSE/MAE)</h3>
        <img src="bar_metrics.png" alt="Bar Metrics">

        <h3>График метрик по слоям</h3>
        <img src="metrics_plot.png" alt="Metrics Plot">

        <h3>CCA-анализ на dev-clean</h3>
        <img src="cca/cca_dev_cps.png" alt="CCA Dev CPS">

        <h3>CCA для модели voxblink2_samresnet34</h3>
        <img src="cca/voxblink2_samresnet34/cca_dev_cps_voxblink2_samresnet34.png" alt="CCA SamResNet34">

        <h3>CCA для модели voxblink2_samresnet34_ft</h3>
        <img src="cca/voxblink2_samresnet34_ft/cca_dev_cps_voxblink2_samresnet34_ft.png" alt="CCA SamResNet34 FT">
        
        <!-- Пример таблицы из CSV (cca_dev_cps.csv) - можно парсить, но для простоты статическая -->
        <h3>Пример данных CCA из cca_dev_cps.csv</h3>
        <table>
            <thead>
                <tr>
                    <th>Layer</th>
                    <th>CCA Correlation</th>
                </tr>
            </thead>
            <tbody>
                <!-- Здесь вставьте данные из CSV, если доступны; пример: -->
                <tr><td>first relu</td><td>0.412</td></tr>
                <tr><td>layer1 relu 1</td><td>0.538</td></tr>
                <!-- ... -->
            </tbody>
        </table>
        
        <p>Метрики сохранены в <code>metrics.txt</code> и предсказания в <code>predictions.csv</code>.</p>
    </div>

    <div class="section">
        <h2>10. Вывод</h2>
        <p>В проведенной работе успешно решена задача оценки представлений скорости речи в эмбеддингах и промежуточных активациях предобученных моделей WeSpeaker на основе датасета LibriSpeech. Скорость речи, вычисленная как количество алфавитно-цифровых символов в транскрипции, деленное на длительность аудио, варьируется в среднем около 22 букв в секунду с небольшими различиями между подмножествами train-clean-100, dev-clean и test-clean. Предобработка данных реализована в скрипте src/preprocess.py, который формирует CSV-файлы с метками CPS для каждого сэмпла, обеспечивая точное соответствие аудиофайлов и целевых значений.</p>
        <p>Извлечение спикерских эмбеддингов размерностью 256 выполнено с использованием модели SimAMResNet34 vb, модифицированным скриптом extract_embeddings.py, с сохранением результатов в data/processed. Обучение MLP для регрессии на этих эмбеддингах в train_model.py с функцией потерь MSE и одним выходным нейроном дало на тестовом множестве MSE около 14.8 и сопоставимый MAE, что указывает на частичное кодирование просодической информации о скорости речи в финальных представлениях спикера. Визуализация t-SNE эмбеддингов с градиентной окраской по CPS подтвердила наличие слабой кластеризации: точки с близкими скоростями речи группируются, но без строгого разделения, что согласуется с умеренной предсказательной способностью модели.</p>
        <p>Послойный анализ, проведенный для четырех моделей WeSpeaker (SimAMResNet34 vb, vb+vc, SimAMResNet100 vb, vc) на подмножестве dev-clean с помощью GetActivations и нелинейного probing MLP, выявил ключевую закономерность: информация о скорости речи наиболее предсказуема в средних слоях сети (10–15 для ResNet34, 12–20 для ResNet100), где MSE достигает минимума (8.9–10.2 в лучших слоях), а затем ухудшается в глубоких слоях, приближаясь к финальным эмбеддингам. Это свидетельствует о постепенной потере просодических характеристик по мере перехода от акустических признаков к абстрактным спикерским идентификаторам. Линейный CCA-анализ дополнил результаты, показав пиковую корреляцию 0.5–0.6 в тех же средних слоях, но с более низкими значениями в целом, подчеркивая необходимость нелинейных преобразований для полного извлечения информации. Модели ResNet100 превосходят ResNet34 по всем метрикам, демонстрируя лучшую сохранность CPS благодаря большей емкости сети.</p>
        <p>Вклад работы заключается в интеграции полного пайплайна — от предобработки и извлечения признаков до послойного probing и визуализации — в форк репозитория с веткой speech_speed, включая модифицированные скрипты, сохраненные модели, метрики, графики и CSV-файлы в results и data/processed. Созданный pull request в оригинальный репозиторий обеспечивает воспроизводимость и расширяемость для других просодических признаков. Ограничения включают анализ только на английском языке без учета пауз в транскрипциях, использование фиксированного паддинга для длинных записей и оценку на относительно малом dev-clean для послойного анализа. Перспективы развития: включение пауз и интонаций в расчет скорости, расширение на многоязычные датасеты, добавление метрик вроде R², а также сравнение с самообученными моделями для оценки влияния задачи спикерской идентификации на просодические представления.</p>
    </div>

</body>
</html>